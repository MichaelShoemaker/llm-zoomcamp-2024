{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_documents = {\n",
    "    # 'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "    # ,\n",
    "    'llm-zoomcamp': '1T3MdwUvqCL3jrh3d3VCXQ8xE0UqRzI3bfgpfBq3ZWG0'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course, file_id in faq_documents.items():\n",
    "    print(course)\n",
    "    course_documents = read_faq(file_id)\n",
    "    documents.append({'course': course, 'documents': course_documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0]['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for course_dict in data[0]['documents']:\n",
    "    # print(course_dict)\n",
    "    course_dict['course'] = 'course llm-zoomcamp'\n",
    "    # previously we used just \"id\" for document ID\n",
    "    course_dict['document_id'] = generate_document_id(course_dict)\n",
    "    documents.append(course_dict)\n",
    "\n",
    "print(len(documents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"document_id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index name: documents_20240819_164418\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "index_name_prefix = 'documents' \n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "print(\"index name:\", index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Answer', 'section': 'Workshops: X', 'question': 'Question', 'course': 'course llm-zoomcamp', 'document_id': '424a2ae1'}\n"
     ]
    }
   ],
   "source": [
    "for doc in data:\n",
    "    for document in doc['documents']:\n",
    "\n",
    "        es_client.index(index=index_name, body=document)\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Answer',\n",
       " 'section': 'Workshops: X',\n",
       " 'question': 'Question',\n",
       " 'course': 'course llm-zoomcamp',\n",
       " 'document_id': '424a2ae1'}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document_ID: d3b7d9dc\n",
      "Question: When is the next cohort?\n",
      "Answer: Summer 2026.\n",
      "\n",
      "Document_ID: 9e5bdc52\n",
      "Question: What is the cosine similarity?\n",
      "Answer: Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.\n",
      "\n",
      "Document_ID: 19c8f553\n",
      "Question: There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\n",
      "Answer: The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings\n",
      "\n",
      "Document_ID: b15ca29d\n",
      "Question: There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\n",
      "Answer: Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")\n",
      "\n",
      "Document_ID: f235b07b\n",
      "Question: I was working on next week’s homework/content - why does it keep changing?\n",
      "Answer: This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.\n",
      "\n",
      "Document_ID: d059e41b\n",
      "Question: What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?\n",
      "Answer: The zoom link is only published to instructors/presenters/TAs.\n",
      "Students participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\n",
      "Don’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.\n",
      "\n",
      "Document_ID: 5ba75c7d\n",
      "Question: Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?\n",
      "Answer: You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\n",
      "Document_ID: f0214e72\n",
      "Question: Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?\n",
      "Answer: Option1: using direnv\n",
      "created the .envrc file & added my API key, ran direnv allow in the terminal\n",
      "was getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "resolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv('.envrc')\n",
      "Option 2: using Codespaces Secrets\n",
      "Log in to your GitHub account and navigate to Settings > Codespaces\n",
      "There is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\n",
      "Once you set this up, the key will be available in your codespaces session\n",
      "\n",
      "Document_ID: 3b018aec\n",
      "Question: TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\n",
      "Answer: Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\n",
      "v_query = embedding_model.encode(query_text)\n",
      "results = search_function(v_query)\n",
      "\n",
      "Document_ID: 806bfdb2\n",
      "Question: OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?\n",
      "Answer: Inside .env file change POSTGRES_HOST=localhost\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"question\": \"When is the next cohort?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es_client.search(index=index_name, body=query)\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Document_ID: {hit['_source']['document_id']}\")\n",
    "    print(f\"Question: {hit['_source']['question']}\")\n",
    "    print(f\"Answer: {hit['_source']['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of indexes:\n",
      "documents_20240819_155907\n",
      "documents_20240819_155813\n"
     ]
    }
   ],
   "source": [
    "indexes = es_client.indices.get_alias(index=\"*\").keys()\n",
    "\n",
    "print(\"List of indexes:\")\n",
    "for index in indexes:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NotFoundError(404, 'index_not_found_exception', 'no such index [documents_20240819_155813]', documents_20240819_155813, index_or_alias)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_95970/435633831.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'documents_20240819_155813'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, force_synthetic_source, from_, highlight, human, ignore_throttled, ignore_unavailable, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, rank, request_cache, rescore, rest_total_hits_as_int, retriever, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version, body)\u001b[0m\n\u001b[1;32m   4117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m__body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4118\u001b[0m             \u001b[0m__headers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4119\u001b[0;31m         return self.perform_request(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   4120\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4121\u001b[0m             \u001b[0m__path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mpath_parts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_parts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         ) as otel_span:\n\u001b[0;32m--> 271\u001b[0;31m             response = self._perform_request(\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(\n\u001b[0m\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: NotFoundError(404, 'index_not_found_exception', 'no such index [documents_20240819_155813]', documents_20240819_155813, index_or_alias)"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"question\": \"When is the next cohort?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es_client.search(index='documents_20240819_155813', body=query)\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Document_ID: {hit['_source']['document_id']}\")\n",
    "    print(f\"Question: {hit['_source']['question']}\")\n",
    "    print(f\"Answer: {hit['_source']['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
